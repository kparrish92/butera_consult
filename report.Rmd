---
title: "Analysis - Pilot Study: Relative intensity of /pt̪k/ and /bd̪ɡ/ in Spanish"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r, include=FALSE}
# Libraries 

library(here)
library(tidyverse)
library(lme4)
library(lmerTest)
library(jtools)

# Load data 

raw_data <- read.csv(here("data", "raw_data.csv"))

```

# Statistical Analysis 

The data were analyzed using generalized mixed effects models in which the outcome variable Relative Intensity (RI) in decibels was predicted by Region (7 regions total), point of articulation (velar, bilabial or dental), sonority (voiced or voiceless), stress (stressed or unstressed), horizontal position (anterior, central or posterior), vertical position (high, mid or low).
Several interactions were also assessed.
Three total 2-way interactions included a region by point of articulation, region by sonority, and a stress by region.
Four total 3-way interactions were assessed: region by place of articulation and sonority, stress by region and sonority, region by sonority by horizontal vowel and finally region by sonority by vertical position. 
Random intercepts by participant were included to take into account the nested structure of the data.
The normality of residuals were determined by visual inspections of a Q-Q and residual versus fitted plots. 
Main effects and interactions were determined by carrying out nested model comparisons. 




```{r, include=FALSE}
mod0 = lmer(Relative.intensity ~ 1 + (1 | Speaker), data = raw_data)
region = lmer(Relative.intensity ~ 1 + Region + (1 | Speaker), 
            data = raw_data)
poa = lmer(Relative.intensity ~ 1 + Region + Point.of.articulation + 
              (1 | Speaker), data = raw_data)
region_poa = lmer(Relative.intensity ~ 1 + Region*Point.of.articulation + 
              (1 | Speaker), data = raw_data)
sonority = lmer(Relative.intensity ~ 1 + Region*Point.of.articulation + 
              Sonority +
              (1 | Speaker), data = raw_data)
region_poa_sonority = lmer(Relative.intensity ~ 1 + Region*Point.of.articulation*
              Sonority +
              (1 | Speaker), data = raw_data)
region_sonority = lmer(Relative.intensity ~ 1 + Region*Sonority + 
                   Point.of.articulation +
                   (1 | Speaker), data = raw_data)
stress = lmer(Relative.intensity ~ 1 + Region*Point.of.articulation*
                  Sonority + Stress +
                  (1 | Speaker), data = raw_data)
stress_region = lmer(Relative.intensity ~ 1 + Region*Point.of.articulation*
                    Sonority + Stress*Region +
                    (1 | Speaker), data = raw_data)
stress_region_sonority = lmer(Relative.intensity ~ 1 + Region*Point.of.articulation*
                        Sonority + Stress*Region*Sonority +
                        (1 | Speaker), data = raw_data)
horizontal = lmer(Relative.intensity ~ 1 + Region*Point.of.articulation*
                     Sonority + Stress*Region*Sonority +
                     Horizontal.position +
                     (1 | Speaker), data = raw_data)
horizontal_sonority_region = lmer(Relative.intensity ~ 1 + Region*Point.of.articulation*
                        Sonority + Stress*Region*Sonority +
                        Horizontal.position*Sonority*Region +
                        (1 | Speaker), data = raw_data)

vertical = lmer(Relative.intensity ~ 1 + Region*Point.of.articulation*
                     Sonority + Stress*Region*Sonority +
                     Horizontal.position*Sonority*Region + 
                     Vertical.Position +
                     (1 | Speaker), data = raw_data)

vertical_sonority_region = lmer(Relative.intensity ~ 1 + Region*Point.of.articulation*
                        Sonority + Stress*Region*Sonority +
                        Horizontal.position*Sonority*Region + 
                        Vertical.Position*Sonority*Region +
                        (1 | Speaker), data = raw_data)

horizontal_region = lmer(Relative.intensity ~ 1 + Region*Point.of.articulation*
                      Sonority + Stress*Region*Sonority +
                      Horizontal.position*Region + 
                      (1 | Speaker), data = raw_data)

vertical_region = lmer(Relative.intensity ~ 1 + Region*Point.of.articulation*
                      Sonority + Stress*Region*Sonority +
                      Horizontal.position*Region + 
                      Vertical.Position*Region + 
                      (1 | Speaker), data = raw_data)

```

```{r}
mod0 = lmer(Relative.intensity ~ 1 + (1 | Speaker), data = raw_data)
region = lmer(Relative.intensity ~ 1 + Region + (1 | Speaker), 
              data = raw_data)
poa = lmer(Relative.intensity ~ 1 + Region + Point.of.articulation + 
             (1 | Speaker), data = raw_data)
sonority = lmer(Relative.intensity ~ 1 + Region + Point.of.articulation + 
                  Sonority +
                  (1 | Speaker), data = raw_data)
stress = lmer(Relative.intensity ~ 1 + Region + Point.of.articulation +
                Sonority + Stress +
                (1 | Speaker), data = raw_data)
horizontal = lmer(Relative.intensity ~ 1 + Region + Point.of.articulation +
                    Sonority + Stress +
                    Horizontal.position +
                    (1 | Speaker), data = raw_data)
vertical = lmer(Relative.intensity ~ 1 + Region + Point.of.articulation +
                    Sonority + Stress +
                  Horizontal.position +
                    Vertical.Position +
                    (1 | Speaker), data = raw_data)
reg_poa = lmer(Relative.intensity ~ 1 + Region*Point.of.articulation +
                  Sonority + Stress +
                Horizontal.position +
                  Vertical.Position +
                  (1 | Speaker), data = raw_data)
reg_son = lmer(Relative.intensity ~ 1 + Region*Point.of.articulation*
               Sonority + Stress +
               Horizontal.position +
               Vertical.Position +
               (1 | Speaker), data = raw_data)
reg_stress = lmer(Relative.intensity ~ 1 + Region*Point.of.articulation*
               Sonority*Stress +
               Horizontal.position +
               Vertical.Position +
               (1 | Speaker), data = raw_data)

```

```{r}
anova_df <- anova(mod0, region, poa, sonority, stress, horizontal, vertical,
      reg_poa, int_2, int_3) %>% 
  rename(p_val = `Pr(>Chisq)`)
 
model = int_3
```

# Results 

Overall, almost every predictor came out as a predictor for RI when the other predictors were held constant. 
You can use these in various places in the manuscript.
I've placed below what predictors had main effects, the numerical report and an interpretation for each.

**Example**
First, there were main effects for: 


## Main effects 

The following main effects were determined using nested model comparisons. 
A main effect suggests that each predictor individually explains some of the variance 

`region` (χ(`r anova_df$Df[2]`) = `r anova_df$Chisq[2] %>% round(digits = 2)`; p < `r anova_df$p_val [2] %>% round(digits = 3)`) - overall, there were regional differences in RI. 

`sonority`(χ(`r anova_df$Df[6]`) = `r anova_df$Chisq[6] %>% round(digits = 2)`; p < `r anova_df$p_val [6] %>% round(digits = 3)`) - overall, voiceless and voiced consonants, were produced with distinct RI.

`Point.of.articulation` (χ(`r anova_df$Df[3]`) = `r anova_df$Chisq[3] %>% round(digits = 2)`; p < `r anova_df$p_val [3] %>% round(digits = 3)`) - did not have a main effect.

`Stress` (χ(`r anova_df$Df[8]`) = `r anova_df$Chisq[8] %>% round(digits = 2)`; p < `r anova_df$p_val [8] %>% round(digits = 3)`) - overall, unstressed syllables had different RI than stressed ones. 

`Horizontal.position` (χ(`r anova_df$Df[11]`) = `r anova_df$Chisq[11] %>% round(digits = 2)`; p < `r anova_df$p_val [11] %>% round(digits = 3)`)-  overall, central vowels were associated with a X db increase in RI, where posterior vowels showed a X 
increase. 

`Vertical.position` - main effect (χ(`r anova_df$Df[15]`) = `r anova_df$Chisq[15] %>% round(digits = 2)`; p < `r anova_df$p_val [15] %>% round(digits = 3)`), so vertical position mattered, but the model dropped one of these coefficients. Did low vowels coincide with some horizontal position?


## Interaction effects 

`Region*sonority` - 2-way interaction (χ(`r anova_df$Df[4]`) = `r anova_df$Chisq[4] %>% round(digits = 2)`; p < `r anova_df$p_val [4] %>% round(digits = 3)`)

`Region*PoA` - **No** 2-way interaction (χ(`r anova_df$Df[5]`) = `r anova_df$Chisq[5] %>% round(digits = 2)`; p < `r anova_df$p_val [5] %>% round(digits = 3)`)

`Stress*region` - 2-way interaction (χ(`r anova_df$Df[9]`) = `r anova_df$Chisq[9] %>% round(digits = 2)`; p < `r anova_df$p_val [9] %>% round(digits = 3)`)

`Horizontal Position*region` - **No** 2-way interaction (χ(`r anova_df$Df[12]`) = `r anova_df$Chisq[12] %>% round(digits = 2)`; p < `r anova_df$p_val [12] %>% round(digits = 3)`)

`Vertical Position*region` - 2-way interaction (χ(`r anova_df$Df[13]`) = `r anova_df$Chisq[13] %>% round(digits = 2)`; p < `r anova_df$p_val [13] %>% round(digits = 3)`)

`Region*poa*sonority` - 3-way interaction (χ(`r anova_df$Df[7]`) = `r anova_df$Chisq[7] %>% round(digits = 2)`; p < `r anova_df$p_val [7] %>% round(digits = 3)`)

`Region*stress*sonority` - **No** 3-way interaction (χ(`r anova_df$Df[10]`) = `r anova_df$Chisq[10] %>% round(digits = 2)`; p < `r anova_df$p_val [10] %>% round(digits = 3)`)

`horizontal*sonority*region` - **No** 3-way interaction (χ(`r anova_df$Df[14]`) = `r anova_df$Chisq[14] %>% round(digits = 2)`; p < `r anova_df$p_val [14] %>% round(digits = 3)`)

`vertical*sonority*region` - 3-way interaction (χ(`r anova_df$Df[16]`) = `r anova_df$Chisq[16] %>% round(digits = 2)`; p < `r anova_df$p_val [16] %>% round(digits = 3)`)

**2-way Region x Sonority interaction**

```{r}
library(emmeans)
emmip(model, Region ~ Sonority)
```


**2-way Region x PoA interaction**

```{r}
library(emmeans)
emmip(model, Region ~ Point.of.articulation)
```

**3-way Region x PoA x Sonority interaction**

```{r}
library(emmeans)
emmip(model, Region ~ Sonority ~ Point.of.articulation)
```

**3-way Region x Stress x Sonority interaction**

```{r}
library(emmeans)
emmip(model, Region ~ Stress ~ Sonority)

```

**2-way Vertical Position x Region**

```{r}
library(emmeans)
emmip(model, Horizontal.position ~ Region)
```

Overall, there were 3 interaction effects. 
First, there was a 2-way region x sonority interaction (X(4) = 69.7; p < 0.005).
This interaction suggests that there were differences in voiced and voiceless consonants per region. 

`add fixef of region then interaction effect to find the difference in RI between voiced (baseline) and voiceless in that region`

According to the model, the effect difference between voiced and voiceless consonants was 12.26 in the case of Chilean Spanish, 

12.26 + 3.06 for Colombian Spanish, 

in Mexican 12.26 - 5.81, 

in North Central Spain 12.26 - 1.44, 

in Peru 12.26 - 2.21 =  10.05

Southern Insular Peninsular 12.26 - 2.22 = 10.04

Insular Spain 12.26 - 1.03150000


Second, there was a 2-way PoA x sonority interaction (X(2) = 12.47; p < 0.005).
This interaction suggests that the differences in RI in voiced and voiceless consonants depended upon the place of articulation across varieties.

Finally, the 3-way interaction region x sonority x place of articulation 
(X(26) = 60.64; p < 0.005) suggests that the RI predicted by region also depends upon whether the consonant was voiced and its place of articulation. 

# sep

Showing evidence of:

RI varies by region overall - main eff of region 
RI varies by sonority overall - main eff of sonority 

Whether each region sees differences in RI by phoneme  - place of articulation 

RI varies as a function of stress  - main eff and fix ef off stress

RI varies by following vowel frontness

RI varies by following vowel height



## General comments and overview

Predictions: /β/ and /ð/ favor a more occluded production following stressed syllables, in contrast with /ɣ/

Evidence: Position*consonant interaction  

Clarification: 
Lower RI = more lentition 

A negative effect of RI per dialect would show evidence of overall lentition in a dialect  

## Statistical Analysis 


## Results 

Nested model comparisons to determine whether RI varies by region, overall point of articulation, and whether variation in POA depends upon region.


P. 24 *For example, velar segments in Colombian and Mexican varieties exhibit higher RI values; however, Peruvian, Southern/Insular Peninsular, and Chilean varieties show higher RI values for bilabial segments, showing differential treatment among velar segments in most varieties investigated.*
*Dental segments show the lowest overall RI values in Peruvian, Mexican and Chilean varieties, whereas velar segments are most lenited in North-Central Peninsular and Southern/Insular Peninsular varieties.*

There is a main effect for **region** (report), but not for PoA nor the interaction.


*P. 25 Breaking down this data further into effect of individual phoneme, some clearer trends emerge. For example, voiceless velar segment /k/ exhibits the lowest RI values among the voiceless series /pt̪k/ for all varieties except Colombian and Chilean Spanish; however, voiced velar segment /ɡ/ shows the highest RI values among the voiced series /bd̪ɡ/ for all varieties except Southern/Insular Peninsular and Chilean. Results are overall still mixed for effect of phoneme on RI according to language variety*

There was main effect for sonority suggesting that, overall, voiced and voiceless segments were produced differently in all dialects. 
There was a two-way interaction between PoA and Region.
This suggests that the effect of PoA depended on /differed by region.
There was a three-way interaction of PoA, region, and sonority. 
This suggests that the differences in sonority based on PoA differed by region. 

*p 27 While the average production of the voiced and voiceless stop consonants in the six varieties of Spanish do not overlap, all varieties do maintain a distinction between the voiced and voiceless series since there is at least a 6.3dB RI difference (= IntDiff) separating the two, shown in the last column of Table 5.*

Baseline is RI when PoA - bilabial, region - chile, sonority - voiceless.
Fixef `PoA` shows differences of all groups relative to bilabial - dental +1.9, velar +.54.
Fixef `region` shows differences of all groups pooled across PoA and sonority.  
- Colombia + 4.13, Mexico + 8.13, Northcentral Spain + 4.42, Peru + 6.95, Insular Peninsular + .36, Insular Spain + 3.39
Fixef `Sonority` shows the difference between the baseline voiced pooled accross PoA and region 

The intercept of the model shows that the mean RI of the dataset when voiced is X, while the fixed effect predictors represent an adjustment to the mean.
In the case of sonority, 8.2 intercept plus 2.26 fixed effect Sonitryvoiceless suggests that the mean RI of voiceness 7.96db difference at the population level.
In other words, accross all varieties, voiced segments show RI difference of 8.24 dB, where voiceless segments have an RI of 2.26db more (10.4db). 

The interaction terms show evidence of differences in RI per region.
The largest effect is Mexico 

One could add the fixed effect parameter estimate for the model to this as well, which would suggest that sonority is assosiated with a difference of (parameter estimate) units (decibels) accross varities. 
The estimate for the RegionMexico by sonority interaction (b = 5.79650) would tell us what adjustment to this effect is made to provide an inferntial estimate of the difference in the minumum variety. 
That is, the model suggests that there is an increase of 5.79650 + 2.26 dB from voiced to voiceless consonants in Mexican Spanish.


# Conclusions and recommendations 
Invite replication due to low samples, note that conclusions are made based on desc stats and that differences may involve sampling issues. 
exploratory nature of analysis 

# Full model 

```{r}
summ(vertical_sonority_region)
```
